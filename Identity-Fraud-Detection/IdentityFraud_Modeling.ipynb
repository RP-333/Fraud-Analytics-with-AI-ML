{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e08bcc-7332-48fd-9ba0-5dc7131d9c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project 3 ‚Äì Identity Fraud Detection\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "To build and evaluate a machine learning model that detects fraudulent behavior by leveraging both transaction and identity-related data. \n",
    "This project focuses on identifying hidden fraud patterns linked to user identities, devices, and behavioral signals.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Dataset Description\n",
    "\n",
    "This project uses a real-world fraud dataset combining two complementary files:\n",
    "\n",
    "- `train_transaction.csv` ‚Äì Contains transaction-level features along with the binary target variable `isFraud`.\n",
    "- `train_identity.csv` ‚Äì Includes additional identity and device metadata linked to each transaction via the `TransactionID` key.\n",
    "` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30164308-0b49-4cad-ae6d-dd34c08a7578",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Starter Code ‚Äì Load & Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c0dc2f4-9d71-40e3-89fc-f855aee87945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### First 5 Rows of `transaction_df`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>...</th>\n",
       "      <th>V330</th>\n",
       "      <th>V331</th>\n",
       "      <th>V332</th>\n",
       "      <th>V333</th>\n",
       "      <th>V334</th>\n",
       "      <th>V335</th>\n",
       "      <th>V336</th>\n",
       "      <th>V337</th>\n",
       "      <th>V338</th>\n",
       "      <th>V339</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 394 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0        2987000        0          86400            68.5         W  13926   \n",
       "1        2987001        0          86401            29.0         W   2755   \n",
       "2        2987002        0          86469            59.0         W   4663   \n",
       "3        2987003        0          86499            50.0         W  18132   \n",
       "4        2987004        0          86506            50.0         H   4497   \n",
       "\n",
       "   card2  card3       card4  card5  ... V330  V331  V332  V333  V334 V335  \\\n",
       "0    NaN  150.0    discover  142.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
       "1  404.0  150.0  mastercard  102.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
       "2  490.0  150.0        visa  166.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
       "3  567.0  150.0  mastercard  117.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
       "4  514.0  150.0  mastercard  102.0  ...  0.0   0.0   0.0   0.0   0.0  0.0   \n",
       "\n",
       "  V336  V337  V338  V339  \n",
       "0  NaN   NaN   NaN   NaN  \n",
       "1  NaN   NaN   NaN   NaN  \n",
       "2  NaN   NaN   NaN   NaN  \n",
       "3  NaN   NaN   NaN   NaN  \n",
       "4  0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 394 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### First 5 Rows of `identity_df`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>id_01</th>\n",
       "      <th>id_02</th>\n",
       "      <th>id_03</th>\n",
       "      <th>id_04</th>\n",
       "      <th>id_05</th>\n",
       "      <th>id_06</th>\n",
       "      <th>id_07</th>\n",
       "      <th>id_08</th>\n",
       "      <th>id_09</th>\n",
       "      <th>...</th>\n",
       "      <th>id_31</th>\n",
       "      <th>id_32</th>\n",
       "      <th>id_33</th>\n",
       "      <th>id_34</th>\n",
       "      <th>id_35</th>\n",
       "      <th>id_36</th>\n",
       "      <th>id_37</th>\n",
       "      <th>id_38</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>DeviceInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70787.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>samsung browser 6.2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2220x1080</td>\n",
       "      <td>match_status:2</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>mobile</td>\n",
       "      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987008</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>98945.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>mobile safari 11.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1334x750</td>\n",
       "      <td>match_status:1</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>mobile</td>\n",
       "      <td>iOS Device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987010</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>191631.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>chrome 62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Windows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2987011</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>221832.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>chrome 62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>desktop</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2987016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7460.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>chrome 62.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1280x800</td>\n",
       "      <td>match_status:2</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>desktop</td>\n",
       "      <td>MacOS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  id_01     id_02  id_03  id_04  id_05  id_06  id_07  id_08  \\\n",
       "0        2987004    0.0   70787.0    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1        2987008   -5.0   98945.0    NaN    NaN    0.0   -5.0    NaN    NaN   \n",
       "2        2987010   -5.0  191631.0    0.0    0.0    0.0    0.0    NaN    NaN   \n",
       "3        2987011   -5.0  221832.0    NaN    NaN    0.0   -6.0    NaN    NaN   \n",
       "4        2987016    0.0    7460.0    0.0    0.0    1.0    0.0    NaN    NaN   \n",
       "\n",
       "   id_09  ...                id_31  id_32      id_33           id_34  id_35  \\\n",
       "0    NaN  ...  samsung browser 6.2   32.0  2220x1080  match_status:2      T   \n",
       "1    NaN  ...   mobile safari 11.0   32.0   1334x750  match_status:1      T   \n",
       "2    0.0  ...          chrome 62.0    NaN        NaN             NaN      F   \n",
       "3    NaN  ...          chrome 62.0    NaN        NaN             NaN      F   \n",
       "4    0.0  ...          chrome 62.0   24.0   1280x800  match_status:2      T   \n",
       "\n",
       "  id_36 id_37  id_38  DeviceType                     DeviceInfo  \n",
       "0     F     T      T      mobile  SAMSUNG SM-G892A Build/NRD90M  \n",
       "1     F     F      T      mobile                     iOS Device  \n",
       "2     F     T      T     desktop                        Windows  \n",
       "3     F     T      T     desktop                            NaN  \n",
       "4     F     T      T     desktop                          MacOS  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### First 5 Rows of Merged Dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>...</th>\n",
       "      <th>id_31</th>\n",
       "      <th>id_32</th>\n",
       "      <th>id_33</th>\n",
       "      <th>id_34</th>\n",
       "      <th>id_35</th>\n",
       "      <th>id_36</th>\n",
       "      <th>id_37</th>\n",
       "      <th>id_38</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>DeviceInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>samsung browser 6.2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2220x1080</td>\n",
       "      <td>match_status:2</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>mobile</td>\n",
       "      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0        2987000        0          86400            68.5         W  13926   \n",
       "1        2987001        0          86401            29.0         W   2755   \n",
       "2        2987002        0          86469            59.0         W   4663   \n",
       "3        2987003        0          86499            50.0         W  18132   \n",
       "4        2987004        0          86506            50.0         H   4497   \n",
       "\n",
       "   card2  card3       card4  card5  ...                id_31  id_32  \\\n",
       "0    NaN  150.0    discover  142.0  ...                  NaN    NaN   \n",
       "1  404.0  150.0  mastercard  102.0  ...                  NaN    NaN   \n",
       "2  490.0  150.0        visa  166.0  ...                  NaN    NaN   \n",
       "3  567.0  150.0  mastercard  117.0  ...                  NaN    NaN   \n",
       "4  514.0  150.0  mastercard  102.0  ...  samsung browser 6.2   32.0   \n",
       "\n",
       "       id_33           id_34  id_35 id_36 id_37  id_38  DeviceType  \\\n",
       "0        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "1        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "2        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "3        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "4  2220x1080  match_status:2      T     F     T      T      mobile   \n",
       "\n",
       "                      DeviceInfo  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4  SAMSUNG SM-G892A Build/NRD90M  \n",
       "\n",
       "[5 rows x 434 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load datasets\n",
    "transaction_df = pd.read_csv(\"train_transaction.csv\")\n",
    "identity_df = pd.read_csv(\"train_identity.csv\")\n",
    "\n",
    "# Merge on TransactionID\n",
    "df = pd.merge(transaction_df, identity_df, how='left', on='TransactionID')\n",
    "\n",
    "# Preview\n",
    "# Display preview of transaction_df\n",
    "display(Markdown(\"### First 5 Rows of `transaction_df`\"))\n",
    "display(transaction_df.head())\n",
    "\n",
    "# Display preview of identity_df\n",
    "display(Markdown(\"### First 5 Rows of `identity_df`\"))\n",
    "display(identity_df.head())\n",
    "\n",
    "# (Optional) Preview of the merged dataset, if already merged\n",
    "display(Markdown(\"### First 5 Rows of Merged Dataset\"))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b57cbc3-97a3-4bc5-b758-08204f2943f2",
   "metadata": {},
   "source": [
    "## Dataset Overview and Context\r\n",
    "\r\n",
    "The dataset used in this assignment is derived from a real-world identity fraud detection challenge. It includes two primary files:\r\n",
    "\r\n",
    "- `train_transaction.csv`: Contains transaction-related features and the fraud label (`isFraud`).\r\n",
    "- `train_identity.csv`: Contains additional identity-related features for a subset of transactions.\r\n",
    "\r\n",
    "### Why Are the Variable Names Not Intuitive?\r\n",
    "\r\n",
    "Many variables in the dataset are anonymized for privacy and proprietary reasons. For example:\r\n",
    "\r\n",
    "- Features are labeled generically (e.g., `V1`, `C1`, `D9`, `id_12`, etc.).\r\n",
    "- This protects sensitive financial information and reflects the kind of datasets you might encounter in industry where full data dictionaries aren't always available.\r\n",
    "\r\n",
    "### How Can We Still Use This Data?\r\n",
    "\r\n",
    "Even though the variable names lack clear definitions, the dataset still contains:\r\n",
    "\r\n",
    "- **Rich signals**: Numeric, categorical, and timestamp-based features.\r\n",
    "- **Ground truth**: A clear target variable (`isFraud`) allows us to train supervised machine learning models.\r\n",
    "- **Structure**: Enough consistency for preprocessing, model training, and evaluation.\r\n",
    "\r\n",
    "We‚Äôll apply standard AI/ML techniques to preprocess, explore, and model the data‚Äîjust like you would in a real-world fraud analytics environment.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17378551-ad08-4d14-99c1-d72d44753d82",
   "metadata": {},
   "source": [
    "### Task 1. Data Merging & Basic Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f34f44-b625-4466-9ba2-7c7c5974d477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the merged dataset (rows, columns):\n",
      "(590540, 434)\n",
      "\n",
      "Preview of the merged dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>...</th>\n",
       "      <th>id_31</th>\n",
       "      <th>id_32</th>\n",
       "      <th>id_33</th>\n",
       "      <th>id_34</th>\n",
       "      <th>id_35</th>\n",
       "      <th>id_36</th>\n",
       "      <th>id_37</th>\n",
       "      <th>id_38</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>DeviceInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>samsung browser 6.2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2220x1080</td>\n",
       "      <td>match_status:2</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>mobile</td>\n",
       "      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0        2987000        0          86400            68.5         W  13926   \n",
       "1        2987001        0          86401            29.0         W   2755   \n",
       "2        2987002        0          86469            59.0         W   4663   \n",
       "3        2987003        0          86499            50.0         W  18132   \n",
       "4        2987004        0          86506            50.0         H   4497   \n",
       "\n",
       "   card2  card3       card4  card5  ...                id_31  id_32  \\\n",
       "0    NaN  150.0    discover  142.0  ...                  NaN    NaN   \n",
       "1  404.0  150.0  mastercard  102.0  ...                  NaN    NaN   \n",
       "2  490.0  150.0        visa  166.0  ...                  NaN    NaN   \n",
       "3  567.0  150.0  mastercard  117.0  ...                  NaN    NaN   \n",
       "4  514.0  150.0  mastercard  102.0  ...  samsung browser 6.2   32.0   \n",
       "\n",
       "       id_33           id_34  id_35 id_36 id_37  id_38  DeviceType  \\\n",
       "0        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "1        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "2        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "3        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "4  2220x1080  match_status:2      T     F     T      T      mobile   \n",
       "\n",
       "                      DeviceInfo  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4  SAMSUNG SM-G892A Build/NRD90M  \n",
       "\n",
       "[5 rows x 434 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage of missing values per column:\n",
      "TransactionID      0.000000\n",
      "isFraud            0.000000\n",
      "TransactionDT      0.000000\n",
      "TransactionAmt     0.000000\n",
      "ProductCD          0.000000\n",
      "                    ...    \n",
      "id_36             76.126088\n",
      "id_37             76.126088\n",
      "id_38             76.126088\n",
      "DeviceType        76.155722\n",
      "DeviceInfo        79.905510\n",
      "Length: 434, dtype: float64\n",
      "\n",
      "Total columns with >75% missing: 208\n",
      "Out of these, identity columns: 40\n"
     ]
    }
   ],
   "source": [
    "### Basic Exploration and checking of missing values in data- \n",
    "\n",
    "# Display shape and preview merged dataset\n",
    "print(\"\\nShape of the merged dataset (rows, columns):\")\n",
    "print(df.shape)\n",
    "\n",
    "print(\"\\nPreview of the merged dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "# Calculate and print the percentage of missing values per column for Merged Dataset -\n",
    "missing_percentage = df.isnull().mean() * 100\n",
    "print(\"\\nPercentage of missing values per column:\")\n",
    "print(missing_percentage)\n",
    "\n",
    "# Get columns with >75% missing in the merged dataset\n",
    "missing_fraction = df.isnull().mean()\n",
    "columns_over_75_missing = missing_fraction[missing_fraction > 0.75].index\n",
    "\n",
    "# Get the list of columns that originally belonged to the identity dataset\n",
    "identity_columns = identity_df.columns.difference(transaction_df.columns)\n",
    "\n",
    "# Find intersection ‚Äî how many of the >75% missing columns are from identity\n",
    "missing_identity_columns = set(columns_over_75_missing).intersection(set(identity_columns))\n",
    "\n",
    "# Output\n",
    "\n",
    "print(f\"\\nTotal columns with >75% missing: {len(columns_over_75_missing)}\")\n",
    "print(f\"Out of these, identity columns: {len(missing_identity_columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc129a8-332c-4246-8fd9-6bab80f65be3",
   "metadata": {},
   "source": [
    "- We explored the merged dataset to assess missing data and identified columns with over 75% missing values. \n",
    "Among these, we will focus on numeric features and compute their correlation with the target variable isFraud.\n",
    "\n",
    "- This will help us detect sparse features that still carry strong predictive signals (correlation > 0.7 or < -0.7), \n",
    "guiding us on which variables to potentially retain for further modeling instead of dropping them outright."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf045a90-d376-4f3b-8273-e31e4c3f868d",
   "metadata": {},
   "source": [
    "#### Check for correlation among numeric features(that has >75% missing values ) before dropping them outrightly !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d13507b-2943-4944-9606-e36ad7aabf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns with >75% missing AND strong correlation with isFraud (>|0.7|):\n",
      "Series([], Name: isFraud, dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "# To make sure we are not dropping valuable information, We are doing Correlation analysis to understand if these variables \n",
    "# have high correlation to isFraud, even though they‚Äôre mostly missing -----\n",
    "\n",
    "# Step: Filter numeric columns from the >75% missing columns\n",
    "numeric_missing_cols = df[columns_over_75_missing].select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Add isFraud for correlation computation\n",
    "temp_df = numeric_missing_cols.copy()\n",
    "temp_df['isFraud'] = df['isFraud']\n",
    "\n",
    "# Compute correlation with isFraud\n",
    "correlation_series = temp_df.corr()['isFraud'].drop('isFraud')\n",
    "\n",
    "# Filter for strong correlation >0.7 or <-0.7\n",
    "strong_corr = correlation_series[correlation_series.abs() > 0.7]\n",
    "\n",
    "# Display results\n",
    "print(\"\\nColumns with >75% missing AND strong correlation with isFraud (>|0.7|):\")\n",
    "print(strong_corr.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27237371-52d7-45f0-80fe-ba1d91d036c7",
   "metadata": {},
   "source": [
    "\n",
    "#### Insights  - \n",
    "Columns with over 75% missing values show no strong correlation (|r| > 0.7) with the target variable isFraud. \n",
    "These sparse features lack significant predictive value for fraud detection and can be safely dropped from the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65944ca-0351-4127-8546-b7b98c2dbf28",
   "metadata": {},
   "source": [
    "#### Subtask- Briefly describe how much of the identity data was missing and how you handled it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bbaba1-bf39-4a31-a36f-c1c97c3c6fcb",
   "metadata": {},
   "source": [
    "\n",
    "Dataset Overview - \n",
    "The above merged dataset provides complete transaction data and nearly complete card-related data. \n",
    "However, identity and device-related features are significantly sparse. \n",
    "\n",
    "Key identity/device fields with substantial missing values include:\n",
    "- id_36, id_37, id_38: ~76% missing\n",
    "- DeviceType: ~76% missing\n",
    "- DeviceInfo: ~80% missing\n",
    "\n",
    "<b>Missing Data Handling Strategy:</b>\n",
    "- Drop features: Remove fields with >75% missing data (listed above).\n",
    "- Impute remaining features: Use mean/median (numerical) or mode (categorical) during processing, based on data type/distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba3e0f6-5c74-48b5-b98e-ccfbcd284b3f",
   "metadata": {},
   "source": [
    "Dataset Overview -\n",
    "Complete transaction data and nearly complete card data are available. Identity/device features show significant sparsity.\n",
    "\n",
    "Key Missing Data (76-80%)\n",
    "\n",
    "id_36, id_37, id_38, DeviceType, DeviceInfo\n",
    "\n",
    "Handling Strategy\n",
    "\n",
    "Drop features: Remove fields with >75% missing data (listed above).\n",
    "\n",
    "Impute remaining features: Use mean/median (numerical) or mode (categorical) during processing, based on data type/distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea13130-ece3-4ffa-aec1-e7303832c09f",
   "metadata": {},
   "source": [
    "### Task 2. Feature Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deabc3f-813b-479f-a615-c1263a877530",
   "metadata": {},
   "source": [
    "### Drop columns with >75% missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92af65c9-5e89-4486-a915-06e7f9886e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropped 208 columns with more than 75.0% missing values.\n",
      "Remaining columns in the dataset: 226\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set threshold\n",
    "threshold = 0.75\n",
    "\n",
    "# Step 2: Calculate missing fraction\n",
    "missing_fraction = df.isnull().mean()\n",
    "\n",
    "# Step 3: Identify columns to drop\n",
    "columns_to_drop = missing_fraction[missing_fraction > threshold].index\n",
    "\n",
    "# Step 4: Drop columns from the merged DataFrame\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Step 5: Output result\n",
    "print(f\"\\nDropped {len(columns_to_drop)} columns with more than {threshold*100}% missing values.\")\n",
    "print(f\"Remaining columns in the dataset: {df.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41367653-51b7-429c-9db4-46c140ce0e95",
   "metadata": {},
   "source": [
    "### Impute missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0ae779-8610-4fd9-8c9c-94bc64ba8ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values imputed: numerical ‚Üí mean, categorical ‚Üí mode.\n",
      "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
      "0        2987000        0          86400            68.5         W  13926   \n",
      "1        2987001        0          86401            29.0         W   2755   \n",
      "2        2987002        0          86469            59.0         W   4663   \n",
      "3        2987003        0          86499            50.0         W  18132   \n",
      "4        2987004        0          86506            50.0         H   4497   \n",
      "\n",
      "        card2  card3       card4  card5  ...   V312  V313  V314  V315  V316  \\\n",
      "0  362.555488  150.0    discover  142.0  ...    0.0   0.0   0.0   0.0   0.0   \n",
      "1  404.000000  150.0  mastercard  102.0  ...    0.0   0.0   0.0   0.0   0.0   \n",
      "2  490.000000  150.0        visa  166.0  ...    0.0   0.0   0.0   0.0   0.0   \n",
      "3  567.000000  150.0  mastercard  117.0  ...  135.0   0.0   0.0   0.0  50.0   \n",
      "4  514.000000  150.0  mastercard  102.0  ...    0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "     V317   V318  V319  V320  V321  \n",
      "0   117.0    0.0   0.0   0.0   0.0  \n",
      "1     0.0    0.0   0.0   0.0   0.0  \n",
      "2     0.0    0.0   0.0   0.0   0.0  \n",
      "3  1404.0  790.0   0.0   0.0   0.0  \n",
      "4     0.0    0.0   0.0   0.0   0.0  \n",
      "\n",
      "[5 rows x 226 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Separate column types\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "# Step 2: Impute numerical columns with mean\n",
    "for col in numerical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "# Step 3: Impute categorical columns with mode\n",
    "for col in categorical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "print(\"Missing values imputed: numerical ‚Üí mean, categorical ‚Üí mode.\")\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e4ea96-dafe-4bad-b66c-322f471e3957",
   "metadata": {},
   "source": [
    "### Cross-checking for successful imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1873838-e14d-4c08-91ad-c03186a5d8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining missing values (if any):\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Check if any missing values remain\n",
    "missing_summary = df.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "# Display top variables (if any) with missing values\n",
    "print(\"\\nRemaining missing values (if any):\")\n",
    "print(missing_summary[missing_summary > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d148844-9595-4f3d-b97a-0bbff0bc95d2",
   "metadata": {},
   "source": [
    "### Label encoding the categorical variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea920233-6965-4126-b8a7-2dbd770c6795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All categorical variables encoded as 0,1,2,... for modeling.\n",
      "\n",
      " Preview of DataFrame after label encoding:\n",
      "   TransactionID  isFraud  TransactionDT  TransactionAmt  ProductCD  card1  \\\n",
      "0        2987000        0          86400            68.5          4  13926   \n",
      "1        2987001        0          86401            29.0          4   2755   \n",
      "2        2987002        0          86469            59.0          4   4663   \n",
      "3        2987003        0          86499            50.0          4  18132   \n",
      "4        2987004        0          86506            50.0          1   4497   \n",
      "\n",
      "        card2  card3  card4  card5  ...   V312  V313  V314  V315  V316  \\\n",
      "0  362.555488  150.0      1  142.0  ...    0.0   0.0   0.0   0.0   0.0   \n",
      "1  404.000000  150.0      2  102.0  ...    0.0   0.0   0.0   0.0   0.0   \n",
      "2  490.000000  150.0      3  166.0  ...    0.0   0.0   0.0   0.0   0.0   \n",
      "3  567.000000  150.0      2  117.0  ...  135.0   0.0   0.0   0.0  50.0   \n",
      "4  514.000000  150.0      2  102.0  ...    0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "     V317   V318  V319  V320  V321  \n",
      "0   117.0    0.0   0.0   0.0   0.0  \n",
      "1     0.0    0.0   0.0   0.0   0.0  \n",
      "2     0.0    0.0   0.0   0.0   0.0  \n",
      "3  1404.0  790.0   0.0   0.0   0.0  \n",
      "4     0.0    0.0   0.0   0.0   0.0  \n",
      "\n",
      "[5 rows x 226 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode each categorical column\n",
    "for col in categorical_cols:\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "print(\"All categorical variables encoded as 0,1,2,... for modeling.\")\n",
    "\n",
    "# Preview first few rows after encoding\n",
    "print(\"\\n Preview of DataFrame after label encoding:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e867b6b-a275-44c9-8bc1-7c7ee007105b",
   "metadata": {},
   "source": [
    "### Task 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ad1897e-f124-48f8-9ba4-0a241e118e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Data split completed. Training samples: 472432  | Testing samples: 118108\n",
      "\n",
      " Data split completed.\n",
      "Training set shape: (472432, 225), Target: (472432,)\n",
      "Test set shape:     (118108, 225), Target: (118108,)\n",
      "\n",
      " Class distribution in training set:\n",
      "isFraud\n",
      "isFraud=0    0.965011\n",
      "isFraud=1    0.034989\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      " Class distribution in test set:\n",
      "isFraud\n",
      "isFraud=0    0.965007\n",
      "isFraud=1    0.034993\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Split the data into training and testing sets (80/20 split)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('isFraud', axis=1)\n",
    "y = df['isFraud']\n",
    "\n",
    "# Perform 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "print(\"\\n Data split completed. Training samples:\", X_train.shape[0], \" | Testing samples:\", X_test.shape[0])\n",
    "\n",
    "# Display shapes\n",
    "print(\"\\n Data split completed.\")\n",
    "print(f\"Training set shape: {X_train.shape}, Target: {y_train.shape}\")\n",
    "print(f\"Test set shape:     {X_test.shape}, Target: {y_test.shape}\")\n",
    "\n",
    "# Display class distribution\n",
    "print(\"\\n Class distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True).rename(lambda x: f\"isFraud={x}\"))\n",
    "\n",
    "print(\"\\n Class distribution in test set:\")\n",
    "print(y_test.value_counts(normalize=True).rename(lambda x: f\"isFraud={x}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c77bd8d7-1e53-47a1-ab53-67a403b74309",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use a Random Forest classifier to train the model and make predictions:\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define and train model\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc3b95-4d64-48f9-b566-495e92e40ca6",
   "metadata": {},
   "source": [
    "### Task 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78cc35da-4367-42b5-89e2-edd96484bb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Confusion Matrix:\n",
      "[[113870    105]\n",
      " [  3289    844]]\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99    113975\n",
      "           1       0.89      0.20      0.33      4133\n",
      "\n",
      "    accuracy                           0.97    118108\n",
      "   macro avg       0.93      0.60      0.66    118108\n",
      "weighted avg       0.97      0.97      0.96    118108\n",
      "\n",
      " ROC-AUC Score: 0.8661\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\n Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\" ROC-AUC Score: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0587b097-d6b7-4913-a550-c90ae2378c63",
   "metadata": {},
   "source": [
    "### Interpretation: Precision vs. Recall (Fraud Detection Context)\n",
    "\n",
    "- Precision (Fraud class = 1): 0.89\n",
    "  Interpretation: When this model predicts a transaction as fraud, it's correct 89% of the time. \n",
    "  This means few false alarms ‚Äî good for user experience and reputation.\n",
    "\n",
    "- Recall (Fraud class = 1): 0.20\n",
    "  Interpretation: This model only caught 20% of all actual fraud cases.\n",
    "  This means it's missing 80% of frauds ‚Äî not acceptable in real-world fraud detection where recall is more critical.\n",
    "\n",
    "Summary:\n",
    "The above model gives - High precision (few false positives), which is Good , and Low recall (many missed frauds) which is Bad.\n",
    "ROC-AUC = 0.8661: That means, Overall model can separate fraud vs. non-fraud reasonably well, but threshold or class balance needs work\n",
    "\n",
    "Therefore,to improve recall, we will now apply SMOTE or undersampling, retrain the model, and compare performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49f5a1-9dd2-4405-9cdc-ffbd9ac71363",
   "metadata": {},
   "source": [
    "### Task 5. Imbalanced Data Handling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb70d1-5a4b-4676-a350-b6df36c3e74f",
   "metadata": {},
   "source": [
    "#### Apply Undersampling the majority class to handle the imbalance.\n",
    "#### Re-train and Re-Evaluate the model performance after applying undersampling technique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "794f8983-b01f-4572-9e2d-21a49e140af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undersampling Applied:\n",
      "isFraud\n",
      "0    16530\n",
      "1    16530\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Evaluation After Undersampling:\n",
      "Confusion Matrix:\n",
      "[[95812 18163]\n",
      " [  964  3169]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.84      0.91    113975\n",
      "           1       0.15      0.77      0.25      4133\n",
      "\n",
      "    accuracy                           0.84    118108\n",
      "   macro avg       0.57      0.80      0.58    118108\n",
      "weighted avg       0.96      0.84      0.89    118108\n",
      "\n",
      "üéØ ROC-AUC After Undersampling: 0.8773\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Undersampling Applied:\")\n",
    "print(y_train_rus.value_counts())\n",
    "\n",
    "# Retrain\n",
    "model_rus = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42)\n",
    "model_rus.fit(X_train_rus, y_train_rus)\n",
    "\n",
    "# Predict\n",
    "y_pred_rus = model_rus.predict(X_test)\n",
    "y_prob_rus = model_rus.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n Evaluation After Undersampling:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rus))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rus))\n",
    "\n",
    "print(f\"üéØ ROC-AUC After Undersampling: {roc_auc_score(y_test, y_prob_rus):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bacbeae-a23f-413c-94ea-6c5cc102a033",
   "metadata": {},
   "source": [
    "#### Comparison and Interpretation of model performance results - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b8279-bc92-4e70-a890-604902a9f71c",
   "metadata": {},
   "source": [
    "| Metric                | Original Model (Imbalanced) | Undersampled Model |\n",
    "| --------------------- | --------------------------- | ------------------ |\n",
    "| **Accuracy**          | 97%                         | 84%                |\n",
    "| **Precision (Fraud)** | 0.89                        | 0.15               |\n",
    "| **Recall (Fraud)**    | 0.20                        | 0.77               |\n",
    "| **F1-Score (Fraud)**  | 0.33                        | 0.25               |\n",
    "| **ROC-AUC**           | 0.8661                      | **0.8773**         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de31804-1d3b-421c-9140-746a3741c9ba",
   "metadata": {},
   "source": [
    "Key Insights:\n",
    "Recall (Fraud) improved significantly (0.20 ‚Üí 0.77)\n",
    "‚Üí The model now detects 77% of fraud cases, compared to just 20% before.\n",
    "\n",
    "ROC-AUC also improved slightly (from 0.8661 ‚Üí 0.8773), indicating better overall separation of fraud vs non-fraud.\n",
    "\n",
    "Trade-Off:\n",
    "Precision (Fraud) dropped (0.89 ‚Üí 0.15)\n",
    "‚Üí This means the model is flagging more legitimate transactions as fraud (i.e., more false positives).\n",
    "\n",
    "Accuracy decreased from 97% to 84% ‚Äî expected, as the model now focuses more on catching frauds.\n",
    "\n",
    "Conclusion:\n",
    "Since catching fraud is critical, the undersampled model is better due to high recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99de5223-168e-4024-b46a-e5f08ada7458",
   "metadata": {},
   "source": [
    "### Enhancement Task 1. Use a Different Model - \n",
    "Here we are training a Gradient Boosting Model on already udersampled data, to which we applied Random Forest Classifier before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5c28df9-39fc-43f1-9270-e244b3507fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/anaconda3/lib/python3.12/site-packages (3.0.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from xgboost) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "## Installing XGBoost for applying Gradient Boosting Model \n",
    "\n",
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8450c2c2-a2b3-4da7-ad0c-e4434e2738d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Undersampled Class Distribution:\n",
      "isFraud\n",
      "0    16530\n",
      "1    16530\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Step 1: Create undersampled training set\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_bal, y_train_bal = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Optional check\n",
    "print(\" Undersampled Class Distribution:\")\n",
    "print(y_train_bal.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaaf0f4-0b09-4009-a464-a91d037e4285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f416bfd8-6fd2-41c6-ab6d-49c853268834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation for XGBoost (Undersampled Training):\n",
      "Confusion Matrix:\n",
      "[[100398  13577]\n",
      " [   774   3359]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.93    113975\n",
      "           1       0.20      0.81      0.32      4133\n",
      "\n",
      "    accuracy                           0.88    118108\n",
      "   macro avg       0.60      0.85      0.63    118108\n",
      "weighted avg       0.96      0.88      0.91    118108\n",
      "\n",
      " ROC-AUC Score: 0.9225\n"
     ]
    }
   ],
   "source": [
    "#### Train XGBoost on Undersampled Data\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "xgb_model = XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "                          eval_metric='logloss', random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Predict on original test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_prob_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\" Evaluation for XGBoost (Undersampled Training):\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(f\" ROC-AUC Score: {roc_auc_score(y_test, y_prob_xgb):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d959a1-36af-4cd6-8027-56ae0f136e69",
   "metadata": {},
   "source": [
    "#### Comparison between XGBoost and Random Forest Classifier(Undersampled Training Set)\n",
    "\n",
    "| Model            | Precision (Fraud) | Recall (Fraud) | F1-Score (Fraud) | ROC-AUC |\n",
    "|------------------|-------------------|----------------|------------------|---------|\n",
    "| Random Forest     | 0.15              | 0.77           | 0.25             | 0.8773  |\n",
    "| XGBoost           | 0.20              | **0.81**       | **0.32**         | **0.9225** |\n",
    "\n",
    "**Conclusion:**  \n",
    "XGBoost performed better than Random Forest by achieving higher recall and ROC-AUC, making it more effective for detecting \n",
    "identity fraud and distinguishing between fraud and non-fraud overall. \n",
    "While its precision dropped slightly, the improved recall makes it more effective for real-world fraud detection where catching fraud is critical.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2863d269-1dab-4641-8a76-b9719d43b9c6",
   "metadata": {},
   "source": [
    "### Enhancement Task2. Ensemble Modeling\n",
    "\n",
    "-Combine two or more models using a voting ensemble or stacked model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42b0adaa-5ef3-4b06-bbeb-2b5ad9ae7e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluation for Voting Ensemble:\n",
      "Confusion Matrix:\n",
      "[[99400 14575]\n",
      " [  842  3291]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.87      0.93    113975\n",
      "           1       0.18      0.80      0.30      4133\n",
      "\n",
      "    accuracy                           0.87    118108\n",
      "   macro avg       0.59      0.83      0.61    118108\n",
      "weighted avg       0.96      0.87      0.91    118108\n",
      "\n",
      " ROC-AUC Score: 0.9091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Create ensemble using soft voting (uses predicted probabilities)\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('rf', model),         # Random Forest trained on undersampled data\n",
    "    ('xgb', xgb_model)     # XGBoost trained on undersampled data\n",
    "], voting='soft')\n",
    "\n",
    "# Train ensemble on undersampled data\n",
    "ensemble_model.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Predict on original test set\n",
    "y_pred_ensemble = ensemble_model.predict(X_test)\n",
    "y_prob_ensemble = ensemble_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "print(\"\\n Evaluation for Voting Ensemble:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_ensemble))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_ensemble))\n",
    "\n",
    "print(f\" ROC-AUC Score: {roc_auc_score(y_test, y_prob_ensemble):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c671ad8d-9963-4110-8f6b-c47b3ae86e80",
   "metadata": {},
   "source": [
    "#### Model Performance Comparison (Undersampled Training Set)\n",
    "\n",
    "| Model            | Precision (Fraud) | Recall (Fraud) | F1-Score (Fraud) | ROC-AUC |\n",
    "|------------------|-------------------|----------------|------------------|---------|\n",
    "| Random Forest     | 0.15              | 0.77           | 0.25             | 0.8773  |\n",
    "| XGBoost           | 0.20              | **0.81**       | **0.32**         | **0.9225** |\n",
    "| Voting Ensemble   | 0.18              | 0.80           | 0.30             | 0.9091  |\n",
    "\n",
    "**Conclusion:**  \n",
    "The Voting Ensemble model performed better than Random Forest and closely followed XGBoost in recall and ROC-AUC. However, it didn't significantly outperform XGBoost and added more complexity, making XGBoost the more efficient and effective choice in this case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46523b69-d31d-4e80-ad87-45cc5f9262ef",
   "metadata": {},
   "source": [
    "Downside - \n",
    "\n",
    "- The main drawback is that the ensemble still suffers from low precision (0.18) ‚Äî meaning many legitimate transactions are incorrectly flagged as fraud. \n",
    "- Also, ensembles are more computationally expensive and harder to interpret. \n",
    "- Minimal Performance Gain - The ensemble had only a slight improvement over Random Forest and did not surpass XGBoost significantly in recall or ROC-AUC.\n",
    "  That means it added complexity without major benefit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9ba0b5-fd1a-400f-917e-6434461b646f",
   "metadata": {},
   "source": [
    "#### Enhancement Task 3. Anomaly Detection Approach\n",
    "\n",
    "Here we are using an unsupervised method -Isolation Forest to detect outliers.\n",
    "Goal - To detect fraud using no labels during training, and evaluate how well the model identifies fraudulent transactions.\n",
    "\n",
    "Isolation Forest identifies outliers based on learned structure, making it valuable for pre-filtering suspicious activity ‚Äî \n",
    "especially in early-stage fraud detection pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af7f2ad3-fb5a-4155-88cc-f6e415c3e725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluation for Isolation Forest (Unsupervised):\n",
      "Confusion Matrix:\n",
      "[[110565   3410]\n",
      " [  3367    766]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97    113975\n",
      "           1       0.18      0.19      0.18      4133\n",
      "\n",
      "    accuracy                           0.94    118108\n",
      "   macro avg       0.58      0.58      0.58    118108\n",
      "weighted avg       0.94      0.94      0.94    118108\n",
      "\n",
      " ROC-AUC Score: 0.5777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# 1. Train Isolation Forest on full training features (unsupervised)\n",
    "iso_model = IsolationForest(n_estimators=100, contamination=0.035, random_state=42)\n",
    "iso_model.fit(X_train)\n",
    "\n",
    "# 2. Predict anomalies on the test set\n",
    "y_pred_iso = iso_model.predict(X_test)   # Output: -1 = anomaly (fraud), 1 = normal\n",
    "\n",
    "# 3. Convert to binary format: 1 = fraud, 0 = non-fraud\n",
    "y_pred_iso_binary = (y_pred_iso == -1).astype(int)\n",
    "\n",
    "# 4. Evaluate against true labels\n",
    "print(\"\\n Evaluation for Isolation Forest (Unsupervised):\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_iso_binary))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_iso_binary))\n",
    "\n",
    "print(f\" ROC-AUC Score: {roc_auc_score(y_test, y_pred_iso_binary):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c1cea0-d088-4b35-85cd-18d7edb13bc7",
   "metadata": {},
   "source": [
    "Q. How well did the anomaly detection model identify fraud?\n",
    "\n",
    "Ans. The Isolation Forest model identified fraud based on outlier behavior without using labels. It achieved a recall of 0.19 and precision of 0.18, meaning it detected some fraud but also produced many false positives. With a ROC-AUC of 0.5777, it‚Äôs not suitable as a standalone model but can be useful as a supporting tool in fraud detection pipelines, especially when labels are unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7394252-2e94-49be-bc29-80f95c4df184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
